<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Vision-Infused Deep Audio Inpainting</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Multi-modality perception is essential to develop interactive intelligence.
    In this work, we consider a new task of visual information-infused audio inpainting, i.e. synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI).">
<meta name="keywords" content="audio inpainting; GAN; audio-visual; deep learning;">
<link rel="author" href="https://hangz-nju-cuhk.github.io/">

<!-- Fonts and stuff -->
<link href="./audio-inpainting/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./audio-inpainting/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./audio-inpainting/iconize.css">
<script async="" src="./audio-inpainting/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">

      <div class="section head">
	<h1>Vision-Infused Deep Audio Inpainting</h1>

	<div class="authors">
    <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://liuziwei7.github.io/">Ziwei Liu</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://sheldontsui.github.io/">Xudong Xu</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://luoping.me/">Ping Luo</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a>
	</div>

	<div class="affiliations">
	  <a href="http://mmlab.ie.cuhk.edu.hk/">Multimedia Laboratory, </a>
	  <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>
	</div>

	<div class="venue">International Conference on Computer Vision (<a href="http://iccv2019.thecvf.com/" target="_blank">ICCV</a>) 2019, </div>
      </div>

      <center><img src="./audio-inpainting/pipeline2.png" border="0" width="90%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
    Multi-modality perception is essential to develop interactive intelligence.
        In this work, we consider a new task of visual information-infused audio inpainting, i.e. synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI).
	</p>
      </div>

<div class="section demo">
	<h2>Demo Video</h2>
	<br>
	<center>
	  <iframe width="810" height="480" src="https://www.youtube.com/embed/2C8s_YuRRxk" frameborder="0" allowfullscreen></iframe>
	</video>
	    </center>
	    </div>

<br>

<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>

          <li class="grid">
	      <div class="griditem">
		<a href="https://hangz-nju-cuhk.github.io/projects/AudioInpainting" target="_blank" class="imageLink"><img src="./audio-inpainting/paper.png"></a><br>
		  <a href="https://hangz-nju-cuhk.github.io/projects/AudioInpainting" target="_blank">Paper</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br>

<div class="section code">
	<h2>Code, Models and Dataset</h2>
	<center>
	  <ul>

          <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/Hangz-nju-cuhk/Vision-Infused-Audio-Inpainter-VIAI" target="_blank" class="imageLink"><img src="./audio-inpainting/code.png"></a><br>
		  <a href="https://github.com/Hangz-nju-cuhk/Vision-Infused-Audio-Inpainter-VIAI" target="_blank">Code, Models and Dataset</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br>

<div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>@inproceedings{zhou2019vision,
  title={Vision-Infused Deep Audio Inpainting},
  author={Zhou, Hang and Liu, Ziwei and Xu, Xudong and Luo, Ping and Wang, Xiaogang},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2019}
}</pre>
	  </div>
      </div>

</body></html>
